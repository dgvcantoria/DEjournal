# Journal — September 13, 2025 — Building Data Pipelines

## 1) What I learned (bullets, not prose)
- Setting up remote connection
- Fact table and Dimension tables in Star schema
- SQL keys: Select, From, Where, Order By, and Limit, Distinct, Count, Aggregations, Group By, Having, Join
- Different kinds of relational database key and what they mean in the database
- Data Normalization and data modelling

## 2) New vocabulary (define in your own words)
- Normalization = creating a new table to include the unique values of a field
- 1NF, 2NF, 3NF/BCNF = different approach of normalization
- OLAP Cubes = 3d relation of fields
- Star Schema = a method of data modelling where in all dimensions are connected to a Fact table without any other dimension table below it

## 3) Data Engineering mindset applied (what principles did I use?)
- Normalize data to prevent insertion, deletion errors

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- When checking what changed in the dataset after applying the dbt, I used left join to match the raw data and the clean data to see if anything was removed with the name of the car as the key field 

## 5) Open questions (things I still don’t get)
- How do we create the dlt and dbt scripts we used?
- Is DBeaver commonly used in the industry?
- How else do we use DBeaver as a tool?

## 6) Next actions (small, doable steps)
- [ ]  Learn more SQL scripts
- [ ]  Redo the process of loading and building
- [ ]  Learn the structure of dlt and dbt used in class

## 7) Artifacts & links (code, queries, dashboards)
- 

---

### Mini reflection (3–5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production?
I was surprised with how fast data cleaning happened in comparison to how we did it in the first 3 sessions. 
### BONUS: What is a meme that best describes what you feel or your learning today?
<img width="500" height="500" alt="image" src="https://github.com/user-attachments/assets/dd7deab0-e164-47ca-a878-3e84de557ad7" />
