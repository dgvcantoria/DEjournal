# Journal — September 20, 2025 — Data Modelling

## 1) What I learned (bullets, not prose)
- Setting up remote connection
- Fact table and Dimension tables in Star schema
- SQL keys: Select, From, Where, Order By, and Limit, Distinct, Count, Aggregations, Group By, Having, Join
- Different kinds of relational database key and what they mean in the database
- Data Normalization and data modelling

## 2) New vocabulary (define in your own words)
- Normalization = creating a new table to include the unique values of a field
- 1NF, 2NF, 3NF/BCNF = different approach of normalization
- OLAP Cubes = 3d relation of fields
- Star Schema = a method of data modelling where in all dimensions are connected to a Fact table without any other dimension table below it

## 3) Data Engineering mindset applied (what principles did I use?)
- Normalize data to prevent insertion, deletion, update errors

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- When deciding what fields to include in Fact and Dimension tables, we decided to keep the dimension table to up to just 1 level with no further dimension table connected to it.

## 5) Open questions (things I still don’t get)
- How do we know if a model is 1NF, 2NF, or 3NF and when should they be used?

## 6) Next actions (small, doable steps)
- [ ]  Learn how to normalize tables specially assigning unique values as keys

## 7) Artifacts & links (code, queries, dashboards)
- 

---

### Mini reflection (3–5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production?
I was surprised of how straight forward the data becomes when doing normalization. However, I did that on dbeaver and it might be more practical to do that together with the dbt when there are more tables to be created. 
### BONUS: What is a meme that best describes what you feel or your learning today?
<img width="500" height="409" alt="image" src="https://github.com/user-attachments/assets/67ba519f-1d6e-43f9-b5f4-a40168677df2" />

